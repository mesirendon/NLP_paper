% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\graphicspath{{img/}}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{hyperref,xcolor}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{fancyvrb}

\usepackage{url}
\urldef{\mailsa}\path|amrendonsa@unal.edu.co|  

\begin{document}
%
\title{Conversational text composition through commonsense knowledge}
%
\titlerunning{Conversational text composition through commonsense knowledge}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Angel Rendon\orcidID{0000-0003-3900-9582}}
%
%\authorrunning{A. Rendon et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{
Universidad Nacional de Colombia\\
\mailsa\\
\url{http://unal.edu.co/}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Natural Language Processing techniques allows us to process text in wide range ways, making possible to extract key information out of texts and even proposing machine translators systems. One of those possibilities is tied to having well trained systems to have smart enough conversations with humans. This work aims to analyze the state-of-the-art techniques and implement them in the construction of a system that using different methods, make it possible to sustain a basic conversation on general topics.

\keywords{commonsense knowledge \and natural language processing \and machine learning \and semantic association}
\end{abstract}
%

\section{Introduction}
One of the artificial intelligence (AI) keystones would be definitively be having fully conversational systems to interact with people for several applications ranging from recommender systems, expert systems, to specialized chatbots and assistants \footnote{\url{https://www.youtube.com/watch?v=d40jgFZ5hXk}}.

Several techniques based on Machine Learning (e.g. Bayesian models, SVM, supervised and unsupervised learning methods), and statistical model methods (e.g. word frequency, text rank, and inverse document frequency), have been used for a long time, with promising results.

However, systems based on these techniques rely on well formed corpora. As an example, WordNet \cite{Miller1990} has the following synset for \emph{cat}:

\begin{Verbatim}
S: (n) computerized tomography, computed tomography, CT,
computerized axial tomography, computed axial tomography, 
CAT (a method of examining body organs by scanning them 
with X rays and using a computer to construct a series of
cross-sectional scans along a single axis) 
\end{Verbatim}

This simple example could lead to think that it would be possible to miss that specific synset when talking about \emph{computerized tomography} when using \emph{cat} in a medical text, showing instead the most probable definition \emph{feline}. That scenario is plausible since the knowledge is strongly dependent on the quality of either unstructured texts or its scale and domain-specific knowledge \cite{Ghazvininejad2017}.

Lexical semantic understanding, sustained by socially shared commonsense knowledge on the core as it has the content of what people intends to know while conversing \cite{Zhou2018}.

This contribution aims to:

\begin{enumerate}
	\item integrates existing work on the semantic association for construction of commonsense knowledge.
	\item build an English conversational system using semantic association based on commonsense knowledge construction techniques.
	\item assess the performance of the built system compared to traditional conversational approaches.
\end{enumerate}

\section{Related work}
A key element to enrich understanding of sentences is tied to word senses. This fact is more important when the word belongs to a specific domain context. Through hybrid clustering techniques \cite{Pantel2004} chooses the best terms elements to build the initial committees (i.e. clusters of meanings for a word) out of the WordNet corpus \cite{Miller1990}, and removing the sense so through further iterations the algorithm can effectively analyze other synsets.

State-of-the-art pre-trained models capture commonsense knowledge with limited value for domain-specific contexts. \cite{Efstathiou2018} proposes a general model for capturing specific domain knowledge of software engineering through a word2vec model exploiting available information on Stack Overflow posts. Their work shows that the model effectively disambiguates metaphorical use of English words when it comes to this specific domain, capturing well grained relations within it.

Distributional Semantic Models are another technique for supporting semantic understanding for natural language processing. These models are highly dependent on the size and quality of the corpora that has the commonsense knowledge for the comprehensive task. While English do have high quality and large scaled commonsense and domain specific information, other languages lack enough material to build comprehensive distributional models. \cite{Barzegar2018} proposes to combine lightweight machine translation model using the English Distributional Semantic Model for building enriched knowledge word vectors for other languages. By building a model leveraged by a unigram-level source-target probabilities which can be directly computed from the parallel corpora, the author gets word vectors for other languages from English DSM as feasible activity, showing up that about 66\% of improvement has been gotten from lightweight models compared to other models. Spanish got the best performance with nearly 60\%, whereas Dutch got the worst with nearly 50\%. This demonstrates that lightweight machine translation models is, in the worst case, equivalent (in some cases outperforming) to the state-of-the-art machine translation systems for translation of word pairs.

As stated at the beginning, one of the important milestones on AI is having intelligent systems to perform common activities as an agent spawned at any location performing a set of actions to answer a question. These actions might require the agent to navigate, process images, understand natural language and learn. \cite{Das2018} proposes a new AI task, EmbodiedQA\footnote{\url{https://embodiedqa.org/}}, where an agent is dropped anywhere and must navigate processing images to gather as much information to answer questions about the environment. To achieve this, the space problem is scoped out using reinforcement learning to reward good trajectories and navigation on well instrumented training environments to provide computer vision, semantic annotations. Their work shows that the navigation problem can be split into two components: The planning (i.e. selecting actions), and the controller (i.e. executing primitive actions); as the \emph{hierarchical navigation module}. Their contribution led to the introduction of the EQA dataset for visual questions of navigation using House3D.

The natural language understanding for stories, as a different kind of narrative, is challenging because it is comprised of a series of events. \cite{Yuan2018} presents a keyword extraction weight based on word-level attention on external knowledge base. External knowledge must be digitalized to guide the encoding process. Then text keyword extraction is done by combining statistically-based Term Frequency and Inverse Document Frequency, and machine-learning-based K-means. Finally an analysis of sentiment is done by a binary tag (positive and negative) using the VADER classifier to estimate the polarity of a sentence in the binary tag numerical value.

This work aims to work on the lexical semantic understanding, but to fully understand what it does imply, awareness about the cognitive issues in natural languages processing must be present. On their work, \cite{Poibeau2016}, address how the cognitive science and the natural language processing fields can help the other by bringing advances on their studies.

``Knowledge can be seen as the result of the confrontation of our a priori
ideas with the reality of the outside world'' \cite{Poibeau2016}. This leads to some problems:

\begin{enumerate}
	\item The task-space is probably infinite, since humans can perceive likewise infinite points of view about things.
	\item Complex inferences on commonsense knowledge result from perception interference on previous gathered information.
	\item The knowledge about how the human brain processes the information is, though promising, very short, and it hardens the proper formalization.
\end{enumerate}

To ease the work commonsense knowledge for natural language processing, a basic assumption must be done about the knowledge: it must be isolated from perception. This assumption led to the creation of the general domain ontology library ConceptNet\cite{Speer2016}. However, these kind of databases lack accuracy formalizing specific domain contexts. Nevertheless, these very same databases have led to amazing discoveries `just' by finding patterns revealed by machine learning means \cite{Poibeau2016}.

On the other hand, both, cognitive science and natural language processing, have evolved independently for more than thirty years, in part because the technology was not properly advanced, in part because there was not enough information as we do currently have, in part because both fields were not well related \cite{Poibeau2016}.

Lately, some research works have shown some convergence of models coming from both study fields like ACT-R theory\footnote{Adaptive Control of Thought-Rational} where the brain work is thought in a modular fashion and how those modules interact to comprehend. In the context of language it does mean a sentence can be analyzed by several modules getting scores of the meaning of words and sentences and how putting all that information together brings more formalized and understood information \cite{Poibeau2016}.

Other works focus on the support of two big ideas: \textbf{Memory} as the `ability to store information and bring it to the front whenever' it's needed \cite{Poibeau2016}, and \textbf{Expectation}, as `the predetermination of a lexical category depending on the context' \cite{Poibeau2016}. This might be treated by research works like \cite{Ma2018}, stating that long short-term memory (LSTM) are convenient for systems to learn implicit knowledge upon data. Other works exploit this idea in order to measure complexity through large-scale probabilistic context-free grammars that consider both dimensions of the problem \cite{Hale2007}.

Other work has focused on the understanding of how children acquire language knowledge by hierarchical Bayesian models, or impact on language abilities produced by clinical conditions as Alzheimer's disease \cite{Poibeau2016}.

Graph theory has been used to model lexicon as an intricate network where words and concepts are seen as nodes interconnected to others by specific relations (e.g. proximity in sentences, synonyms, hypernyms hyponyms, synsets, etc) \cite{Poibeau2016}.

Although not consider for this work, awareness must be raised for the very adaptive nature of languages. Languages evolve to use new coined words, giving new meanings to existing ones, adopting terms from other dialects or languages \cite{Poibeau2016}.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{/home/mesi/Documents/BibTex/NLP.bib}
%
%\begin{thebibliography}{8}
%\bibitem{ref_lncs1}
%Author, F., Author, S.: Title of a proceedings paper. In: Editor,
%F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
%Springer, Heidelberg (2016). \doi{10.10007/1234567890}
%\end{thebibliography}
\end{document}
